{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS1sM42qASgwpTnuOmnogN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasterBeard/EigenCluster/blob/main/EigenCluster_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lUqlDXiP_mU",
        "outputId": "a6e4497c-3133-4767-f27a-ca76073ac82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (23092, 44)\n",
            "Train norm_features shape: (23092, 44)\n",
            "Train labels shape: (23092,)\n",
            "Validation features shape: (2410, 44)\n",
            "Validation norm_features shape: (2410, 44)\n",
            "Validation labels shape: (2410,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# Define index tickers\n",
        "index_tickers = {\n",
        "    'SPX': '^GSPC',     # S&P 500\n",
        "    'IXIC': '^IXIC',    # NASDAQ Composite\n",
        "    'HSI': '^HSI',      # Hang Seng Index\n",
        "    'DJI': '^DJI',      # Dow Jones Industrial Average\n",
        "    'FCHI': '^FCHI',    # CAC 40\n",
        "    'DAXI': '^GDAXI',   # DAX\n",
        "    'N225': '^N225',    # Nikkei 225\n",
        "    'KS11': '^KS11',    # KOSPI\n",
        "    'SENSEX': '^BSESN', # BSE Sensex\n",
        "    'STOXX50': '^STOXX50E',  # EURO STOXX 50\n",
        "}\n",
        "\n",
        "date_ranges = {\n",
        "    'train': (\"2000-01-01\", \"2009-12-31\"),\n",
        "    'val': (\"2010-01-02\", \"2010-12-31\")\n",
        "}\n",
        "\n",
        "# Store feature matrices and labels for each split (including norm_features)\n",
        "data_splits = {split: {'features': [], 'norm_features': [], 'labels': []} for split in date_ranges}\n",
        "\n",
        "# Window size\n",
        "window_size = 11\n",
        "\n",
        "# Fetch and process data for each time period\n",
        "for split, (start_date, end_date) in date_ranges.items():\n",
        "    index_data = {}\n",
        "    for name, ticker in index_tickers.items():\n",
        "        if ticker == '000001.SS':\n",
        "            index_data[name] = yf.Ticker(ticker).history(start=start_date, end=end_date, auto_adjust=True)\n",
        "        else:\n",
        "            index_data[name] = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "    # Create feature vectors and labels for each index\n",
        "    for index_name, data in index_data.items():\n",
        "        if data.empty:\n",
        "            continue\n",
        "\n",
        "        # Fix multi-level column names (only happens with Ticker().history())\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "        open_values = data['Open'].dropna().values\n",
        "        close_values = data['Close'].dropna().values\n",
        "        low_values = data['Low'].dropna().values\n",
        "        high_values = data['High'].dropna().values\n",
        "\n",
        "        for start in range(len(data) - window_size + 1):\n",
        "            open_row = open_values[start:start + window_size]\n",
        "            low_row = low_values[start:start + window_size]\n",
        "            high_row = high_values[start:start + window_size]\n",
        "            close_row = close_values[start:start + window_size]\n",
        "\n",
        "            # Build open-low-high-close-volume feature vector (not normalized)\n",
        "            combined = np.array([\n",
        "                val for i in range(window_size)\n",
        "                for val in (open_row[i], low_row[i], high_row[i], close_row[i])\n",
        "            ])\n",
        "\n",
        "            # Normalized feature vector\n",
        "            norm_combined = np.array([\n",
        "                (open_row[i] / close_row[-2],\n",
        "                low_row[i] / close_row[-2],\n",
        "                high_row[i] / close_row[-2],\n",
        "                close_row[i] / close_row[-2])\n",
        "                for i in range(window_size)\n",
        "            ]).flatten()\n",
        "\n",
        "            label = 1 if close_row[-1] > close_row[-2] else 0\n",
        "\n",
        "            # Store results\n",
        "            data_splits[split]['features'].append(combined)\n",
        "            data_splits[split]['norm_features'].append(norm_combined)\n",
        "            data_splits[split]['labels'].append(label)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "train_features = np.array(data_splits['train']['features'])\n",
        "train_norm_features = np.array(data_splits['train']['norm_features'])\n",
        "train_labels = np.array(data_splits['train']['labels'])\n",
        "\n",
        "val_features = np.array(data_splits['val']['features'])\n",
        "val_norm_features = np.array(data_splits['val']['norm_features'])\n",
        "val_labels = np.array(data_splits['val']['labels'])\n",
        "\n",
        "# Output shapes of each set\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Train norm_features shape: {train_norm_features.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "\n",
        "print(f\"Validation features shape: {val_features.shape}\")\n",
        "print(f\"Validation norm_features shape: {val_norm_features.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ============================================\n",
        "# Utility functions\n",
        "# ============================================\n",
        "def extract_normalized_daily_ohlc(matrices, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Each window is normalized by its 5th-to-last value\n",
        "\n",
        "    Args:\n",
        "        matrices: Input feature matrices\n",
        "        epsilon: Small value to avoid division by zero\n",
        "\n",
        "    Returns:\n",
        "        Normalized window data as numpy array\n",
        "    \"\"\"\n",
        "    all_windows = []\n",
        "\n",
        "    for vec in matrices:\n",
        "        reshaped = vec.reshape(-1, 4)  # shape: (11, 4)\n",
        "        for i in range(1, reshaped.shape[0] - 10 + 1):  # Total 7 windows\n",
        "            window = reshaped[i:i + 10]  # shape: (5, 4)\n",
        "            flat = window.flatten()      # shape: (20,)\n",
        "\n",
        "            denominator = flat[-5]  # 5th-to-last value (flat[15], day 4's close)\n",
        "            if abs(denominator) < epsilon:\n",
        "                denominator = 1.0  # Avoid division by zero\n",
        "\n",
        "            normalized = flat / denominator\n",
        "            all_windows.append(normalized)\n",
        "\n",
        "    return np.array(all_windows)  # shape: [N_samples Ã— 7, 20]\n",
        "\n",
        "# Process training and validation data\n",
        "train_days = extract_normalized_daily_ohlc(train_features)\n",
        "val_days = extract_normalized_daily_ohlc(val_features)"
      ],
      "metadata": {
        "id": "JY-ufZfdQVwU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
        "\n",
        "# 1. Principal Component Analysis (PCA) - variance maximization\n",
        "# Initialize PCA with all components (same dimension as input)\n",
        "pca = PCA(n_components=train_days.shape[1])\n",
        "\n",
        "# Transform training and validation data using PCA\n",
        "train_1d = pca.fit_transform(train_days)  # Fit and transform training data\n",
        "val_1d = pca.transform(val_days)          # Transform validation data (using same PCA model)\n",
        "\n",
        "# Create features for clustering:\n",
        "# Combine mean of sine values with vector norm as a new feature\n",
        "train_theta = (np.mean(np.sin(train_1d), axis=1) * np.linalg.norm(train_1d, axis=1)).reshape(-1, 1)\n",
        "val_theta = (np.mean(np.sin(val_1d), axis=1) * np.linalg.norm(val_1d, axis=1)).reshape(-1, 1)\n",
        "\n",
        "# Perform K-means clustering with 15 clusters\n",
        "kmeans = KMeans(n_clusters=15, random_state=0)  # Using fixed random state for reproducibility\n",
        "train_clusters = kmeans.fit_predict(train_theta)  # Fit on training data\n",
        "val_clusters = kmeans.predict(val_theta)         # Predict on validation data\n",
        "\n",
        "# Get cluster centers (shape: (n_clusters, 1) since we're using 1D features)\n",
        "centers = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "MAqFvmgjQozp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get all unique cluster centers (sorted)\n",
        "unique_clusters = np.unique(train_clusters)  # All cluster indices\n",
        "\n",
        "# Store class 1 ratio and sample count for each cluster\n",
        "cluster_info = []\n",
        "\n",
        "# Calculate statistics for each cluster\n",
        "for cluster in unique_clusters:\n",
        "    # Create mask for current cluster\n",
        "    cluster_mask = (train_clusters == cluster)\n",
        "    cluster_labels = train_labels[cluster_mask]\n",
        "\n",
        "    # Calculate class statistics\n",
        "    n_total = len(cluster_labels)\n",
        "    n_class1 = np.sum(cluster_labels == 1)\n",
        "    class1_ratio = n_class1 / n_total\n",
        "\n",
        "    # Store cluster information\n",
        "    cluster_info.append({\n",
        "        \"cluster\": cluster,\n",
        "        \"class1_ratio\": class1_ratio,\n",
        "        \"n_class1\": n_class1,\n",
        "        \"n_class0\": n_total - n_class1,\n",
        "        \"n_total\": n_total\n",
        "    })\n",
        "\n",
        "# Custom sorting key function\n",
        "def sort_key(info):\n",
        "    \"\"\"\n",
        "    Sorting key function that prioritizes:\n",
        "    1. Clusters with >50% class 1 (sorted by n_class1 ascending)\n",
        "    2. Other clusters (sorted by n_class0 descending)\n",
        "    \"\"\"\n",
        "    ratio = info[\"class1_ratio\"]\n",
        "    if ratio > 0.5:\n",
        "        # For >50% class 1: sort by n_class1 ascending (smaller samples rank higher)\n",
        "        return (0, info[\"n_class1\"])  # 0 indicates high priority group\n",
        "    else:\n",
        "        # For â‰¤50% class 1: sort by n_class0 descending (smaller samples rank lower)\n",
        "        return (1, -info[\"n_class0\"])  # 1 indicates low priority group\n",
        "\n",
        "# Sort clusters using custom rules\n",
        "sorted_clusters = sorted(cluster_info, key=sort_key)\n",
        "\n",
        "# Print results\n",
        "print(\"Clusters sorted by custom rules:\")\n",
        "for info in sorted_clusters:\n",
        "    cluster = info[\"cluster\"]\n",
        "    ratio = info[\"class1_ratio\"]\n",
        "    n_class0 = info[\"n_class0\"]\n",
        "    n_class1 = info[\"n_class1\"]\n",
        "    n_total = info[\"n_total\"]\n",
        "\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    print(f\"  Class 0 count: {n_class0}\")\n",
        "    print(f\"  Class 1 count: {n_class1}\")\n",
        "    print(f\"  Class 1 ratio: {ratio:.2%}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eicRVIjvQ7a3",
        "outputId": "cbbba689-b22d-4517-b58c-b4120f830183"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters sorted by custom rules:\n",
            "Cluster 1:\n",
            "  Class 0 count: 5\n",
            "  Class 1 count: 17\n",
            "  Class 1 ratio: 77.27%\n",
            "---\n",
            "Cluster 12:\n",
            "  Class 0 count: 18\n",
            "  Class 1 count: 39\n",
            "  Class 1 ratio: 68.42%\n",
            "---\n",
            "Cluster 2:\n",
            "  Class 0 count: 54\n",
            "  Class 1 count: 81\n",
            "  Class 1 ratio: 60.00%\n",
            "---\n",
            "Cluster 6:\n",
            "  Class 0 count: 93\n",
            "  Class 1 count: 151\n",
            "  Class 1 ratio: 61.89%\n",
            "---\n",
            "Cluster 5:\n",
            "  Class 0 count: 208\n",
            "  Class 1 count: 342\n",
            "  Class 1 ratio: 62.18%\n",
            "---\n",
            "Cluster 8:\n",
            "  Class 0 count: 539\n",
            "  Class 1 count: 661\n",
            "  Class 1 ratio: 55.08%\n",
            "---\n",
            "Cluster 0:\n",
            "  Class 0 count: 1493\n",
            "  Class 1 count: 1693\n",
            "  Class 1 ratio: 53.14%\n",
            "---\n",
            "Cluster 14:\n",
            "  Class 0 count: 5405\n",
            "  Class 1 count: 5865\n",
            "  Class 1 ratio: 52.04%\n",
            "---\n",
            "Cluster 9:\n",
            "  Class 0 count: 1947\n",
            "  Class 1 count: 1919\n",
            "  Class 1 ratio: 49.64%\n",
            "---\n",
            "Cluster 4:\n",
            "  Class 0 count: 774\n",
            "  Class 1 count: 752\n",
            "  Class 1 ratio: 49.28%\n",
            "---\n",
            "Cluster 11:\n",
            "  Class 0 count: 328\n",
            "  Class 1 count: 311\n",
            "  Class 1 ratio: 48.67%\n",
            "---\n",
            "Cluster 7:\n",
            "  Class 0 count: 134\n",
            "  Class 1 count: 109\n",
            "  Class 1 ratio: 44.86%\n",
            "---\n",
            "Cluster 13:\n",
            "  Class 0 count: 56\n",
            "  Class 1 count: 46\n",
            "  Class 1 ratio: 45.10%\n",
            "---\n",
            "Cluster 3:\n",
            "  Class 0 count: 27\n",
            "  Class 1 count: 13\n",
            "  Class 1 ratio: 32.50%\n",
            "---\n",
            "Cluster 10:\n",
            "  Class 0 count: 7\n",
            "  Class 1 count: 5\n",
            "  Class 1 ratio: 41.67%\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep middle 36 features (remove first 4 and last 4 columns)\n",
        "train_x = train_norm_features[:, 4:40]  # Shape: [n_samples, 36]\n",
        "val_x = val_norm_features[:, 4:40]     # Shape: [n_samples, 36]\n",
        "\n",
        "# Use cluster assignments as labels\n",
        "y_train = train_clusters\n",
        "y_val = val_clusters\n",
        "\n",
        "# Verify new shapes\n",
        "print(\"Train features new shape:\", train_x.shape)\n",
        "print(\"Validation features new shape:\", val_x.shape)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Define models with consistent configurations\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=150,\n",
        "        max_depth=None,              # No maximum depth\n",
        "        min_samples_split=2,         # Minimum samples to split node\n",
        "        min_samples_leaf=2,          # Minimum samples at leaf node\n",
        "        max_features='sqrt',         # Features to consider at each split\n",
        "        random_state=42              # For reproducibility\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,                 # Maximum tree depth\n",
        "        learning_rate=0.1,           # Boosting learning rate\n",
        "        subsample=1.0,              # Subsample ratio of training instances\n",
        "        colsample_bytree=0.8,        # Subsample ratio of features\n",
        "        random_state=42,\n",
        "        # ðŸ”´ Key parameters for multi-class classification\n",
        "        objective='multi:softmax',   # Multiclass objective function\n",
        "        num_class=len(np.unique(y_train)),  # Number of classes\n",
        "        eval_metric='mlogloss',      # Multiclass evaluation metric\n",
        "        early_stopping_rounds=10     # Early stopping rounds\n",
        "    )\n",
        "}\n",
        "\n",
        "# 2. Unified training process (with explicit validation set)\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "\n",
        "    if model_name == \"XGBoost\":\n",
        "        # XGBoost with early stopping on validation set\n",
        "        model.fit(\n",
        "            train_x, y_train,\n",
        "            eval_set=[(val_x, y_val)],\n",
        "            verbose=False\n",
        "        )\n",
        "    else:\n",
        "        # Standard fit for other models\n",
        "        model.fit(train_x, y_train)\n",
        "\n",
        "# 3. Evaluation function for cluster analysis\n",
        "def evaluate_cluster_range(cluster_indices, true_labels, pred_labels):\n",
        "    \"\"\"\n",
        "    Evaluate performance on specific clusters\n",
        "\n",
        "    Args:\n",
        "        cluster_indices: List of cluster indices to evaluate\n",
        "        true_labels: Ground truth labels\n",
        "        pred_labels: Predicted cluster assignments\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (mean accuracy, sample count) or (None, 0) if no samples\n",
        "    \"\"\"\n",
        "    mask = np.isin(pred_labels, cluster_indices)\n",
        "    y_subset = true_labels[mask]\n",
        "    return (np.mean(y_subset), len(y_subset)) if len(y_subset) > 0 else (None, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UeeyvLyRPxy",
        "outputId": "e5fb5485-4233-4455-f551-be562a65aa04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features new shape: (23092, 36)\n",
            "Validation features new shape: (2410, 36)\n",
            "\n",
            "=== Training RandomForest ===\n",
            "\n",
            "=== Training XGBoost ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "successful_stocks = 0  # Counter for successfully processed stocks\n",
        "\n",
        "# File path configuration\n",
        "zip_path = '/content/drive/My Drive/SP500_data2010-2020.zip'\n",
        "extract_dir = '/content/SP500_data2010-2020'\n",
        "\n",
        "# Extract zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "def read_sp500_csv(filepath):\n",
        "    \"\"\"\n",
        "    Read S&P 500 CSV file and filter data for 2011-2020 period\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the CSV file\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing OHLC data for the specified period\n",
        "    \"\"\"\n",
        "    # Read CSV with multi-level headers and drop the second level\n",
        "    df = pd.read_csv(filepath, header=[0,1], index_col=0, parse_dates=True)\n",
        "    df.columns = df.columns.droplevel(1)\n",
        "\n",
        "    # Keep only required columns\n",
        "    required_cols = ['Open', 'High', 'Low', 'Close']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    # Set date range filter (2011-2020)\n",
        "    start_date = pd.Timestamp(\"2011-01-01\", tz=\"UTC\") if df.index.tz else pd.Timestamp(\"2011-01-01\")\n",
        "    end_date = pd.Timestamp(\"2020-12-31\", tz=\"UTC\") if df.index.tz else pd.Timestamp(\"2020-12-31\")\n",
        "\n",
        "    # Filter data by date range\n",
        "    mask = (df.index >= start_date) & (df.index <= end_date)\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    return df[required_cols]\n",
        "\n",
        "# Main processing pipeline\n",
        "window_size = 11  # Size of the rolling window\n",
        "test_features1 = []  # Raw features storage\n",
        "test_norm_features1 = []  # Normalized features storage\n",
        "test_labels1 = []  # Labels storage (1=up, 0=down)\n",
        "\n",
        "# Get all CSV files in directory\n",
        "csv_files = sorted([f for f in os.listdir(extract_dir) if f.endswith('.csv')])\n",
        "print(f\"Found {len(csv_files)} S&P 500 constituent stock files\")\n",
        "\n",
        "for csv_file in csv_files[:]:  # Process all files\n",
        "    try:\n",
        "        filepath = os.path.join(extract_dir, csv_file)\n",
        "\n",
        "        # Read and process CSV file\n",
        "        ohlc = read_sp500_csv(filepath)\n",
        "        if len(ohlc['Open'].values) != 0:\n",
        "            successful_stocks += 1\n",
        "\n",
        "        # Skip if no data in date range\n",
        "        if len(ohlc) == 0:\n",
        "            print(f\"{csv_file} has no data in 2011-2020 period\")\n",
        "            continue\n",
        "\n",
        "        # Extract OHLC values\n",
        "        open_vals = ohlc['Open'].values\n",
        "        high_vals = ohlc['High'].values\n",
        "        low_vals = ohlc['Low'].values\n",
        "        close_vals = ohlc['Close'].values\n",
        "\n",
        "        # Create rolling window features\n",
        "        for start in range(len(ohlc) - window_size):\n",
        "            window_open = open_vals[start:start+window_size]\n",
        "            window_high = high_vals[start:start+window_size]\n",
        "            window_low = low_vals[start:start+window_size]\n",
        "            window_close = close_vals[start:start+window_size]\n",
        "\n",
        "            # Create feature vector (Open, Low, High, Close order)\n",
        "            feature_vec = np.array([\n",
        "                val for i in range(window_size)\n",
        "                for val in (window_open[i], window_low[i], window_high[i], window_close[i])\n",
        "            ])\n",
        "\n",
        "            # Create label (1 if price increased, 0 otherwise)\n",
        "            label = 1 if window_close[-1] > window_close[-2] else 0\n",
        "\n",
        "            test_features1.append(feature_vec)\n",
        "            test_norm_features1.append(feature_vec/window_close[-2])  # Normalized by previous close\n",
        "            test_labels1.append(label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {csv_file}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Convert to numpy arrays\n",
        "if test_features1:  # Check if any data was processed\n",
        "    test_features1 = np.array(test_features1)\n",
        "    test_norm_features1 = np.array(test_norm_features1)\n",
        "    test_labels1 = np.array(test_labels1)\n",
        "else:\n",
        "    test_features1 = np.array([])\n",
        "    test_labels1 = np.array([])\n",
        "    test_norm_features1 = np.array([])\n",
        "\n",
        "# Output results\n",
        "print(\"\\nTest set generation results:\")\n",
        "if len(test_features1) > 0:\n",
        "    print(f\"Feature matrix shape: {test_features1.shape}\")\n",
        "    print(f\"Normalized feature matrix shape: {test_norm_features1.shape}\")\n",
        "    print(f\"Successfully processed stocks: {successful_stocks}\")\n",
        "    print(f\"Label distribution - Up: {np.mean(test_labels1):.1%}, Down: {1-np.mean(test_labels1):.1%}\")\n",
        "else:\n",
        "    print(\"No features generated - please check input files or date range\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEWRu1DjRWXR",
        "outputId": "f52f1d81-7da0-4d86-9ac9-dc7364c37d74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 501 S&P 500 constituent stock files\n",
            "AMTM.csv has no data in 2011-2020 period\n",
            "CEG.csv has no data in 2011-2020 period\n",
            "GEHC.csv has no data in 2011-2020 period\n",
            "GEV.csv has no data in 2011-2020 period\n",
            "KVUE.csv has no data in 2011-2020 period\n",
            "SOLV.csv has no data in 2011-2020 period\n",
            "SW.csv has no data in 2011-2020 period\n",
            "VLTO.csv has no data in 2011-2020 period\n",
            "\n",
            "Test set generation results:\n",
            "Feature matrix shape: (1176149, 44)\n",
            "Normalized feature matrix shape: (1176149, 44)\n",
            "Successfully processed stocks: 493\n",
            "Label distribution - Up: 52.2%, Down: 47.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data for the current index\n",
        "test_x = test_norm_features1[:, 4:40]  # Select middle 36 features (remove first 4 and last 4)\n",
        "test_labels = test_labels1  # Ground truth labels\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "\n",
        "    # Make predictions on test set\n",
        "    y_pred = model.predict(test_x)\n",
        "\n",
        "    # Store predictions for evaluation\n",
        "    test_pred1 = y_pred\n",
        "\n",
        "    # Evaluate top performing clusters\n",
        "    print(\"Top Cluster Evaluation:\")\n",
        "    for n in range(1, 8):  # Evaluate top 1-7 clusters\n",
        "        # Get cluster IDs of top n clusters\n",
        "        clusters = sorted_clusters[:n]\n",
        "        clusters = [item['cluster'] for item in clusters]\n",
        "\n",
        "        # Evaluate performance on these clusters\n",
        "        acc, count = evaluate_cluster_range(clusters, test_labels, test_pred1)\n",
        "\n",
        "        if acc is not None:\n",
        "            # Print accuracy and sample count for these clusters\n",
        "            print(f\"Top {n} clusters: Acc={acc:.4f}, Samples={count}\")\n",
        "        else:\n",
        "            print(f\"Top {n} clusters: No samples\")\n",
        "\n",
        "    # Evaluate bottom performing clusters\n",
        "    print(\"\\nBottom Cluster Evaluation:\")\n",
        "    for n in range(1, 8):  # Evaluate bottom 1-7 clusters\n",
        "        # Get cluster IDs of bottom n clusters\n",
        "        clusters = sorted_clusters[-n:]\n",
        "        clusters = [item['cluster'] for item in clusters]\n",
        "\n",
        "        # Evaluate performance on these clusters\n",
        "        acc, count = evaluate_cluster_range(clusters, test_labels, test_pred1)\n",
        "\n",
        "        if acc is not None:\n",
        "            # Print inverse accuracy (1-acc) since these are worst performers\n",
        "            print(f\"Bottom {n} clusters: Acc={(1-acc):.4f}, Samples={count}\")\n",
        "        else:\n",
        "            print(f\"Bottom {n} clusters: No samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxyUQVJcRwNj",
        "outputId": "9812dcfd-9187-4a52-b2d6-eb54149bbf19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: RandomForest\n",
            "Top Cluster Evaluation:\n",
            "Top 1 clusters: Acc=0.5922, Samples=3136\n",
            "Top 2 clusters: Acc=0.5746, Samples=7007\n",
            "Top 3 clusters: Acc=0.5619, Samples=15735\n",
            "Top 4 clusters: Acc=0.5511, Samples=29829\n",
            "Top 5 clusters: Acc=0.5461, Samples=56120\n",
            "Top 6 clusters: Acc=0.5401, Samples=109646\n",
            "Top 7 clusters: Acc=0.5369, Samples=248244\n",
            "\n",
            "Bottom Cluster Evaluation:\n",
            "Bottom 1 clusters: Acc=0.6041, Samples=2900\n",
            "Bottom 2 clusters: Acc=0.5626, Samples=7385\n",
            "Bottom 3 clusters: Acc=0.5354, Samples=16751\n",
            "Bottom 4 clusters: Acc=0.5158, Samples=35170\n",
            "Bottom 5 clusters: Acc=0.5036, Samples=75567\n",
            "Bottom 6 clusters: Acc=0.4986, Samples=166374\n",
            "Bottom 7 clusters: Acc=0.4907, Samples=370209\n",
            "\n",
            "Model: XGBoost\n",
            "Top Cluster Evaluation:\n",
            "Top 1 clusters: Acc=0.6249, Samples=2141\n",
            "Top 2 clusters: Acc=0.5714, Samples=6683\n",
            "Top 3 clusters: Acc=0.5603, Samples=15192\n",
            "Top 4 clusters: Acc=0.5521, Samples=29607\n",
            "Top 5 clusters: Acc=0.5464, Samples=54948\n",
            "Top 6 clusters: Acc=0.5405, Samples=107763\n",
            "Top 7 clusters: Acc=0.5370, Samples=244340\n",
            "\n",
            "Bottom Cluster Evaluation:\n",
            "Bottom 1 clusters: Acc=0.5855, Samples=3040\n",
            "Bottom 2 clusters: Acc=0.5572, Samples=6985\n",
            "Bottom 3 clusters: Acc=0.5291, Samples=17030\n",
            "Bottom 4 clusters: Acc=0.5150, Samples=36580\n",
            "Bottom 5 clusters: Acc=0.5039, Samples=79204\n",
            "Bottom 6 clusters: Acc=0.4980, Samples=174637\n",
            "Bottom 7 clusters: Acc=0.4903, Samples=377975\n"
          ]
        }
      ]
    }
  ]
}